{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Any', 'BaseLLM', 'Callable', 'Dict', 'Type', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__getattr__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_import_ai21', '_import_aleph_alpha', '_import_amazon_api_gateway', '_import_anthropic', '_import_anyscale', '_import_arcee', '_import_aviary', '_import_azure_openai', '_import_azureml_endpoint', '_import_baidu_qianfan_endpoint', '_import_bananadev', '_import_baseten', '_import_beam', '_import_bedrock', '_import_bittensor', '_import_cerebriumai', '_import_chatglm', '_import_clarifai', '_import_cohere', '_import_ctransformers', '_import_ctranslate2', '_import_databricks', '_import_deepinfra', '_import_deepsparse', '_import_edenai', '_import_fake', '_import_fireworks', '_import_forefrontai', '_import_gigachat', '_import_google_palm', '_import_gooseai', '_import_gpt4all', '_import_gradient_ai', '_import_huggingface_endpoint', '_import_huggingface_hub', '_import_huggingface_pipeline', '_import_huggingface_text_gen_inference', '_import_human', '_import_javelin_ai_gateway', '_import_koboldai', '_import_llamacpp', '_import_manifest', '_import_minimax', '_import_mlflow_ai_gateway', '_import_modal', '_import_mosaicml', '_import_nlpcloud', '_import_octoai_endpoint', '_import_ollama', '_import_opaqueprompts', '_import_openai', '_import_openai_chat', '_import_openllm', '_import_openlm', '_import_pai_eas_endpoint', '_import_petals', '_import_pipelineai', '_import_predibase', '_import_predictionguard', '_import_promptlayer', '_import_promptlayer_chat', '_import_replicate', '_import_rwkv', '_import_sagemaker_endpoint', '_import_self_hosted', '_import_self_hosted_hugging_face', '_import_stochasticai', '_import_symblai_nebula', '_import_textgen', '_import_titan_takeoff', '_import_titan_takeoff_pro', '_import_together', '_import_tongyi', '_import_vertex', '_import_vertex_model_garden', '_import_vllm', '_import_vllm_openai', '_import_writer', '_import_xinference', '_import_yandex_gpt', 'anthropic', 'bananadev', 'base', 'bedrock', 'cerebriumai', 'cohere', 'forefrontai', 'get_type_to_cls_dict', 'gigachat', 'gooseai', 'huggingface_hub', 'huggingface_pipeline', 'huggingface_text_gen_inference', 'llamacpp', 'loading', 'minimax', 'modal', 'ollama', 'openai', 'petals', 'pipelineai', 'sagemaker_endpoint', 'stochasticai', 'utils', 'vertexai', 'writer', 'yandex']\n"
     ]
    }
   ],
   "source": [
    "from langchain import llms\n",
    "\n",
    "print(dir(llms))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPEN_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(openai_api_key=os.environ[\"OPEN_API_KEY\"], temperature= 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?\n",
      "\n",
      "There is no single definitive answer to this question as opinions vary greatly on who the best basketball player is. Some popular contenders include LeBron James, Michael Jordan, Kareem Abdul-Jabbar, and Kobe Bryant.\n"
     ]
    }
   ],
   "source": [
    "text = 'Who is the best basketball player'\n",
    "print(llm.predict(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The capital of India is New Delhi.\n"
     ]
    }
   ],
   "source": [
    "text = 'What is the capital of india'\n",
    "print(llm.predict(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]=\"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dhivakar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '0.19.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "llm_huggingface =  HuggingFaceHub(repo_id=\"google/flan-t5-base\", model_kwargs={\"temperature\":0, \"max_length\":64})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'smolensk'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = llm_huggingface.predict(\"what is the capital of russia\")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yes'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = llm_huggingface.predict(\"Is lebron James is the best basketball player\")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'185 cm'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = llm_huggingface.predict(\"what Is lebron James height\")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i love a samurai warrior'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = llm_huggingface.predict(\"Can you write a poem about AI\")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nAIs so smart, they can learn and think\\nProgrammed to solve problems in a blink\\n\\nTheir capabilities are ever expanding\\nOur future with them is demanding\\n\\nThey can help us with countless tasks\\nWe can trust them to take the lead and ask\\n\\nNo matter how hard the problem may be\\nAIs will work it out, you'll see\\n\\nTheir logic and reasoning is so advanced\\nWe'll be able to rest, safe in the knowledge that our lives are enhanced\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict(\"Can you write a poem about AI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prompt template and llmchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me the capital of this India'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "prompt_template = PromptTemplate(input_variables=['capital'], \n",
    "template = 'Tell me the capital of this {country}')\n",
    "prompt_template.format(country='India' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The capital of India is New Delhi.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain \n",
    "chain = LLMChain(llm=llm, prompt = prompt_template)\n",
    "print(chain.run(\"India\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combining mulitplechains using simple sequential hain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMChain(prompt=PromptTemplate(input_variables=[''], template='Please tell me the capital of the country {}'), llm=OpenAI(client=<class 'openai.api_resources.completion.Completion'>, temperature=0.6, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy=''))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capital_template = PromptTemplate(input_variables=['country'],\n",
    "template = 'Please tell me the capital of the country {}')\n",
    "capital_chain = LLMChain(llm = llm,prompt= capital_template)\n",
    "capital_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['capital'], template='suggest me some amzing places to visit in {capital}')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "famous_template=PromptTemplate(input_variables=['capital'],\n",
    "                               template = \"suggest me some amzing places to visit in {capital}\")\n",
    "famous_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "famous_chain = LLMChain(llm = llm, prmpt = famopus_template) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
